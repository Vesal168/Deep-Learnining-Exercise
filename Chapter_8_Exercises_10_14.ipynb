{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 10"
      ],
      "metadata": {
        "id": "_EuY0fOYcOdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "\n",
        "# Define the input data\n",
        "data = np.array([[[[1, 1], [1, 1]], [[0, 0], [0, 0]]],\n",
        "                 [[[1, 0], [1, 0]], [[0, 1], [0, 1]]],\n",
        "                 [[[1, 0], [0, 1]], [[0, 1], [1, 0]]],\n",
        "                 [[[0, 0], [1, 1]], [[1, 1], [0, 0]]]])\n",
        "\n",
        "# Define the corresponding class labels\n",
        "labels = np.array(['Solid', 'Vertical', 'Diagonal', 'Horizontal'])\n",
        "\n",
        "# Convert the input data to the required format\n",
        "X = data.astype('float32')\n",
        "\n",
        "# Normalize the pixel values to the range of [0, 1]\n",
        "X /= 1.0\n",
        "\n",
        "# Convert the class labels to numerical values\n",
        "label_mapping = {'Solid': 0, 'Vertical': 1, 'Diagonal': 2, 'Horizontal': 3}\n",
        "y = np.array([label_mapping[label] for label in labels])\n",
        "\n",
        "# Convert the class labels to one-hot encoding\n",
        "num_classes = len(np.unique(y))\n",
        "y = np.eye(num_classes)[y]\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=X.shape[1:]))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=200, batch_size=2)\n",
        "\n",
        "# Test the model\n",
        "test_data = np.array([[[[1, 0], [0, 1]], [[0, 1], [1, 0]]],\n",
        "                      [[[0, 0], [1, 1]], [[1, 1], [0, 0]]]])\n",
        "\n",
        "test_X = test_data.astype('float32')\n",
        "test_X /= 1.0\n",
        "\n",
        "predictions = model.predict(test_X)\n",
        "predicted_labels = [list(label_mapping.keys())[np.argmax(prediction)] for prediction in predictions]\n",
        "print(\"Predicted labels:\", predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLXh4CC4cS5t",
        "outputId": "e6d54f11-e988-4500-b2d3-f605ddbfb725"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 7ms/step - loss: 1.3298 - accuracy: 0.2500\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.3188 - accuracy: 0.2500\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.3088 - accuracy: 0.2500\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.3028 - accuracy: 0.2500\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2930 - accuracy: 0.2500\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2832 - accuracy: 0.2500\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.2757 - accuracy: 0.2500\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.2684 - accuracy: 0.2500\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2601 - accuracy: 0.2500\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2504 - accuracy: 0.2500\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2450 - accuracy: 0.2500\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2357 - accuracy: 0.5000\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2306 - accuracy: 0.5000\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2226 - accuracy: 0.5000\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2142 - accuracy: 0.5000\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2071 - accuracy: 0.5000\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.2002 - accuracy: 0.5000\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1932 - accuracy: 0.5000\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.1863 - accuracy: 0.5000\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1799 - accuracy: 0.5000\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1741 - accuracy: 0.5000\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.1667 - accuracy: 0.5000\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1581 - accuracy: 0.5000\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.1513 - accuracy: 0.5000\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1459 - accuracy: 0.5000\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1388 - accuracy: 0.5000\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1314 - accuracy: 0.7500\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1250 - accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1177 - accuracy: 0.7500\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1113 - accuracy: 0.7500\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.1046 - accuracy: 0.7500\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0967 - accuracy: 0.7500\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.0903 - accuracy: 0.7500\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0849 - accuracy: 0.7500\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0782 - accuracy: 0.7500\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0722 - accuracy: 0.7500\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0642 - accuracy: 0.7500\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0592 - accuracy: 0.7500\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0521 - accuracy: 0.7500\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0454 - accuracy: 0.7500\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0390 - accuracy: 0.7500\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0334 - accuracy: 0.7500\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.0266 - accuracy: 0.7500\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0215 - accuracy: 0.7500\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0158 - accuracy: 0.7500\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0102 - accuracy: 0.7500\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.0044 - accuracy: 0.7500\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.9991 - accuracy: 0.7500\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9929 - accuracy: 0.7500\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9869 - accuracy: 0.7500\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9819 - accuracy: 0.7500\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9758 - accuracy: 0.7500\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9702 - accuracy: 0.7500\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9651 - accuracy: 0.7500\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9594 - accuracy: 0.7500\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9539 - accuracy: 0.7500\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9494 - accuracy: 0.7500\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9431 - accuracy: 0.7500\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9379 - accuracy: 0.7500\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9327 - accuracy: 0.7500\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9279 - accuracy: 0.7500\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9235 - accuracy: 0.7500\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9182 - accuracy: 0.7500\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9122 - accuracy: 0.7500\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9077 - accuracy: 0.7500\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9027 - accuracy: 0.7500\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.8969 - accuracy: 0.7500\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.8919 - accuracy: 0.7500\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.8879 - accuracy: 0.7500\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8818 - accuracy: 0.7500\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8773 - accuracy: 0.7500\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8726 - accuracy: 0.7500\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8670 - accuracy: 0.7500\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8626 - accuracy: 0.7500\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8575 - accuracy: 0.7500\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8529 - accuracy: 0.7500\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.8477 - accuracy: 0.7500\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8427 - accuracy: 0.7500\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8378 - accuracy: 0.7500\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8332 - accuracy: 0.7500\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.8283 - accuracy: 0.7500\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8231 - accuracy: 0.7500\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8185 - accuracy: 0.7500\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.8138 - accuracy: 0.7500\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.8088 - accuracy: 0.7500\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.8045 - accuracy: 0.7500\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7994 - accuracy: 0.7500\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7946 - accuracy: 0.7500\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7896 - accuracy: 0.7500\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7854 - accuracy: 0.7500\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7805 - accuracy: 0.7500\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7758 - accuracy: 0.7500\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7711 - accuracy: 0.7500\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7666 - accuracy: 0.7500\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7621 - accuracy: 0.7500\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7571 - accuracy: 0.7500\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7532 - accuracy: 0.7500\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7480 - accuracy: 0.7500\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7435 - accuracy: 0.7500\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7390 - accuracy: 0.7500\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7343 - accuracy: 0.7500\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7298 - accuracy: 0.7500\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7259 - accuracy: 0.7500\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7216 - accuracy: 0.7500\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7163 - accuracy: 0.7500\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7123 - accuracy: 0.7500\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7079 - accuracy: 0.7500\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7034 - accuracy: 0.7500\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6988 - accuracy: 0.7500\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6944 - accuracy: 0.7500\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6903 - accuracy: 0.7500\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6856 - accuracy: 0.7500\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6813 - accuracy: 0.7500\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6771 - accuracy: 0.7500\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6729 - accuracy: 0.7500\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6684 - accuracy: 0.7500\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6645 - accuracy: 0.7500\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6597 - accuracy: 0.7500\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6555 - accuracy: 0.7500\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6518 - accuracy: 0.7500\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6475 - accuracy: 0.7500\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6433 - accuracy: 0.7500\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6397 - accuracy: 0.7500\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6354 - accuracy: 0.7500\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6307 - accuracy: 0.7500\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6267 - accuracy: 0.7500\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6230 - accuracy: 0.7500\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6194 - accuracy: 0.7500\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6149 - accuracy: 0.7500\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6109 - accuracy: 0.7500\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6064 - accuracy: 0.7500\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6024 - accuracy: 0.7500\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5990 - accuracy: 0.7500\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5950 - accuracy: 0.7500\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5908 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5873 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5830 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5791 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5754 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5716 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5682 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5642 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5607 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5569 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5535 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5499 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5462 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5422 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5391 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5351 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5316 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5286 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5248 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5210 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5178 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5145 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5112 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5075 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5038 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5004 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4974 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4942 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4906 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4873 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4839 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4809 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4774 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4745 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4712 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4679 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4647 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4619 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4588 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4555 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4521 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4494 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4464 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4430 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4401 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4372 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4344 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4313 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4286 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4255 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4226 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4194 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4168 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4137 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4111 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4083 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4055 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.4029 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4000 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3970 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3947 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3920 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3891 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3865 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3838 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3813 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "Predicted labels: ['Diagonal', 'Horizontal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 11"
      ],
      "metadata": {
        "id": "-TvB8ePdcYG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Flatten the images\n",
        "X_train_flatten = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test_flatten = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "# Basic MLP model\n",
        "mlp_model = Sequential()\n",
        "mlp_model.add(Dense(128, activation='relu', input_shape=(X_train_flatten.shape[1],)))\n",
        "mlp_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Deep neural network model\n",
        "dnn_model = Sequential()\n",
        "dnn_model.add(Dense(256, activation='relu', input_shape=(X_train_flatten.shape[1],)))\n",
        "dnn_model.add(Dense(128, activation='relu'))\n",
        "dnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the models\n",
        "mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the models\n",
        "mlp_model.fit(X_train_flatten, y_train, epochs=10, batch_size=128, verbose=1)\n",
        "dnn_model.fit(X_train_flatten, y_train, epochs=10, batch_size=128, verbose=1)\n",
        "\n",
        "# Evaluate the models on the test set\n",
        "mlp_loss, mlp_accuracy = mlp_model.evaluate(X_test_flatten, y_test, verbose=0)\n",
        "dnn_loss, dnn_accuracy = dnn_model.evaluate(X_test_flatten, y_test, verbose=0)\n",
        "\n",
        "print(\"MLP Model - Test Loss:\", mlp_loss)\n",
        "print(\"MLP Model - Test Accuracy:\", mlp_accuracy)\n",
        "\n",
        "print(\"Deep Neural Network Model - Test Loss:\", dnn_loss)\n",
        "print(\"Deep Neural Network Model - Test Accuracy:\", dnn_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4IAudZ3cbD-",
        "outputId": "19a78124-ec93-4eac-f23c-3b188425d270"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5493 - accuracy: 0.8120\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4079 - accuracy: 0.8572\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3640 - accuracy: 0.8701\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3382 - accuracy: 0.8776\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3224 - accuracy: 0.8837\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3051 - accuracy: 0.8892\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2923 - accuracy: 0.8944\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2834 - accuracy: 0.8968\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.2703 - accuracy: 0.9019\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2607 - accuracy: 0.9044\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 6ms/step - loss: 0.5084 - accuracy: 0.8198\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3670 - accuracy: 0.8669\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3270 - accuracy: 0.8797\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.3047 - accuracy: 0.8871\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2830 - accuracy: 0.8957\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2713 - accuracy: 0.8997\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2571 - accuracy: 0.9048\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.2435 - accuracy: 0.9089\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2357 - accuracy: 0.9109\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2269 - accuracy: 0.9136\n",
            "MLP Model - Test Loss: 0.33729204535484314\n",
            "MLP Model - Test Accuracy: 0.8791000247001648\n",
            "Deep Neural Network Model - Test Loss: 0.32587626576423645\n",
            "Deep Neural Network Model - Test Accuracy: 0.8865000009536743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 12"
      ],
      "metadata": {
        "id": "6IAaUQJncpN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_classes = len(np.unique(y))\n",
        "y_train_encoded = to_categorical(y_train, num_classes)\n",
        "y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Basic MLP model\n",
        "mlp_model = Sequential()\n",
        "mlp_model.add(Dense(10, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "mlp_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Deep neural network model\n",
        "dnn_model = Sequential()\n",
        "dnn_model.add(Dense(10, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "dnn_model.add(Dense(10, activation='relu'))\n",
        "dnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the models\n",
        "mlp_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the models\n",
        "mlp_model.fit(X_train_scaled, y_train_encoded, epochs=100, batch_size=16, verbose=0)\n",
        "dnn_model.fit(X_train_scaled, y_train_encoded, epochs=100, batch_size=16, verbose=0)\n",
        "\n",
        "# Evaluate the models on the test set\n",
        "mlp_loss, mlp_accuracy = mlp_model.evaluate(X_test_scaled, y_test_encoded, verbose=0)\n",
        "dnn_loss, dnn_accuracy = dnn_model.evaluate(X_test_scaled, y_test_encoded, verbose=0)\n",
        "\n",
        "print(\"MLP Model - Test Loss:\", mlp_loss)\n",
        "print(\"MLP Model - Test Accuracy:\", mlp_accuracy)\n",
        "\n",
        "print(\"Deep Neural Network Model - Test Loss:\", dnn_loss)\n",
        "print(\"Deep Neural Network Model - Test Accuracy:\", dnn_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0ermOHPcrTm",
        "outputId": "1454ce95-80be-482e-c12e-f2b77a92cbed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Model - Test Loss: 0.13140229880809784\n",
            "MLP Model - Test Accuracy: 1.0\n",
            "Deep Neural Network Model - Test Loss: 0.05497350916266441\n",
            "Deep Neural Network Model - Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 13"
      ],
      "metadata": {
        "id": "8UWoqmEfc363"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "url = \"/content/winequality-white.csv\"\n",
        "data = pd.read_csv(url, delimiter=\";\")\n",
        "\n",
        "# Split the data into input features and output variable\n",
        "X = data.drop(\"quality\", axis=1)\n",
        "y = data[\"quality\"]\n",
        "\n",
        "# Preprocessing\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = to_categorical(y)  # Convert the output variable to one-hot encoding for multi-class classification\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation=\"relu\", input_shape=(11,)))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))  # Assuming 10 classes for quality (0-9)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Make predictions on new data\n",
        "new_samples = pd.DataFrame({\n",
        "    \"fixed acidity\": [7.4, 7.8, 7.8, 11.2, 7.4],\n",
        "    \"volatile acidity\": [0.7, 0.88, 0.76, 0.28, 0.7],\n",
        "    \"citric acid\": [0, 0, 0.04, 0.56, 0],\n",
        "    \"residual sugar\": [1.9, 2.6, 2.3, 1.9, 1.9],\n",
        "    \"chlorides\": [0.076, 0.098, 0.092, 0.075, 0.076],\n",
        "    \"free sulfur dioxide\": [11, 25, 15, 17, 11],\n",
        "    \"total sulfur dioxide\": [34, 67, 54, 60, 34],\n",
        "    \"density\": [0.9978, 0.9968, 0.997, 0.998, 0.9978],\n",
        "    \"pH\": [3.51, 3.2, 3.26, 3.16, 3.51],\n",
        "    \"sulphates\": [0.56, 0.68, 0.65, 0.58, 0.56],\n",
        "    \"alcohol\": [9.4, 9.8, 9.8, 9.8, 9.4]\n",
        "})\n",
        "\n",
        "new_samples_scaled = scaler.transform(new_samples)\n",
        "predictions = model.predict(new_samples_scaled)\n",
        "predicted_qualities = [round(prediction.argmax()) for prediction in predictions]\n",
        "\n",
        "print(\"Predicted Qualities:\", predicted_qualities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulEyA_NNc7LV",
        "outputId": "6d08d77a-95d9-4a73-da97-bb28947e1623"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.4188 - accuracy: 0.4883 - val_loss: 1.1872 - val_accuracy: 0.5286\n",
            "Epoch 2/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 1.1223 - accuracy: 0.5521 - val_loss: 1.1287 - val_accuracy: 0.5306\n",
            "Epoch 3/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 1.0757 - accuracy: 0.5664 - val_loss: 1.0997 - val_accuracy: 0.5459\n",
            "Epoch 4/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 1.0512 - accuracy: 0.5625 - val_loss: 1.0989 - val_accuracy: 0.5388\n",
            "Epoch 5/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 1.0361 - accuracy: 0.5804 - val_loss: 1.0795 - val_accuracy: 0.5480\n",
            "Epoch 6/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 1.0217 - accuracy: 0.5717 - val_loss: 1.0751 - val_accuracy: 0.5561\n",
            "Epoch 7/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 1.0101 - accuracy: 0.5786 - val_loss: 1.0615 - val_accuracy: 0.5561\n",
            "Epoch 8/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.9985 - accuracy: 0.5868 - val_loss: 1.0649 - val_accuracy: 0.5347\n",
            "Epoch 9/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.9870 - accuracy: 0.5842 - val_loss: 1.0490 - val_accuracy: 0.5531\n",
            "Epoch 10/10\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 0.9777 - accuracy: 0.5870 - val_loss: 1.0479 - val_accuracy: 0.5541\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 1.0479 - accuracy: 0.5541\n",
            "Test Loss: 1.0478867292404175\n",
            "Test Accuracy: 0.5540816187858582\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "Predicted Qualities: [4, 4, 4, 5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 14"
      ],
      "metadata": {
        "id": "UxOIFt2PekHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load the dataset\n",
        "url = \"/content/seeds_dataset.txt\"\n",
        "column_names = [\"A\", \"P\", \"C\", \"LK\", \"WK\", \"A_Coef\", \"LKG\", \"target\"]\n",
        "data = pd.read_csv(url, delimiter=\"\\t\", header=None, names=column_names)\n",
        "\n",
        "# Split the data into input features and output variable\n",
        "segments = data.drop(\"target\", axis=1)\n",
        "labels = data[\"target\"]\n",
        "\n",
        "X = np.array(segments)\n",
        "y = np.array(labels)\n",
        "# Verify the shapes of X and y\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "# Encode the activity labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)\n",
        "# y = to_categorical(y)\n",
        "# # Preprocessing\n",
        "# if X.shape[1] > 0:\n",
        "#     scaler = StandardScaler()\n",
        "#     X = scaler.fit_transform(X.values)\n",
        "#     y = to_categorical(y.values)\n",
        "# else:\n",
        "#     print(\"No input features found. Please verify the dataset.\")\n",
        "# print(X.shape[1])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_shape=(X.shape[1],)))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.002)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Make predictions on new data\n",
        "new_samples1 = pd.DataFrame({\n",
        "    \"A\": [14.88, 13.91, 14.89],\n",
        "    \"P\": [14.57, 13.73, 14.55],\n",
        "    \"C\": [0.8811, 0.8952, 0.888],\n",
        "    \"LK\": [5.554, 5.291, 5.324],\n",
        "    \"WK\": [3.333, 3.337, 3.379],\n",
        "    \"A_Coef\": [2.676, 1.879, 2.281],\n",
        "    \"LKG\": [5.001, 4.825, 5.147],\n",
        "})\n",
        "\n",
        "new_samples_scaled1 = scaler.transform(new_samples1)\n",
        "predictions = model.predict(new_samples_scaled1)\n",
        "predicted_species = [prediction.argmax() + 1 for prediction in predictions]\n",
        "\n",
        "print(\"Predicted Species:\", predicted_species)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QMhTlC_Ienzs",
        "outputId": "69f96267-ab1d-48d2-a947-36d037a4466a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (210, 7)\n",
            "Shape of y: (210,)\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 1s 40ms/step - loss: 0.3477 - accuracy: 0.3333 - val_loss: -0.2666 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1776 - accuracy: 0.3333 - val_loss: -0.4903 - val_accuracy: 0.3333\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1468 - accuracy: 0.3333 - val_loss: -0.6172 - val_accuracy: 0.3333\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1117 - accuracy: 0.3333 - val_loss: -0.6472 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0649 - accuracy: 0.3333 - val_loss: -0.6337 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0078 - accuracy: 0.3333 - val_loss: -0.6282 - val_accuracy: 0.3333\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: -0.0501 - accuracy: 0.3333 - val_loss: -0.6840 - val_accuracy: 0.3333\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: -0.0973 - accuracy: 0.3333 - val_loss: -0.7420 - val_accuracy: 0.3333\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 9ms/step - loss: -0.1440 - accuracy: 0.3333 - val_loss: -0.7279 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 12ms/step - loss: -0.2026 - accuracy: 0.3333 - val_loss: -0.8125 - val_accuracy: 0.3333\n",
            "2/2 [==============================] - 0s 8ms/step - loss: -0.8125 - accuracy: 0.3333\n",
            "Test Loss: -0.8125432133674622\n",
            "Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c1b2d4f250bc>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m })\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mnew_samples_scaled1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_samples1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_samples_scaled1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mpredicted_species\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    993\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- A\n- A_Coef\n- C\n- LK\n- LKG\n- ...\nFeature names seen at fit time, yet now missing:\n- alcohol\n- chlorides\n- citric acid\n- density\n- fixed acidity\n- ...\n"
          ]
        }
      ]
    }
  ]
}